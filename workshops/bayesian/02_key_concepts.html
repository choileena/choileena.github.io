<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Become a Bayesian with R &amp; Stan" />
<meta property="og:type" content="book" />


<meta property="og:description" content="An introduction to using R for Bayesian data analysis." />
<meta name="github-repo" content="m-clark/Workshops" />


<meta name="date" content="2016-12-11" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="An introduction to using R for Bayesian data analysis.">

<title>Become a Bayesian with R &amp; Stan</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="toc_test.css" type="text/css" />
<link rel="stylesheet" href="notebooks.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#home"><span style="color:transparent">Home</span></a></li>
<li class="has-sub"><a href="01_intro.html#introduction">Introduction</a><ul>
<li><a href="01_intro.html#outline">Outline</a></li>
<li><a href="01_intro.html#prerequisites">Prerequisites</a></li>
</ul></li>
<li class="has-sub"><a href="02_key_concepts.html#key-concepts">Key Concepts</a><ul>
<li><a href="02_key_concepts.html#distributions">Distributions</a></li>
<li><a href="02_key_concepts.html#prior">Prior</a></li>
<li><a href="02_key_concepts.html#likelihood">Likelihood</a></li>
<li><a href="02_key_concepts.html#posterior">Posterior</a></li>
<li><a href="02_key_concepts.html#p-values">P-values</a></li>
</ul></li>
<li class="has-sub"><a href="03_stan.html#stan">Stan</a><ul>
<li><a href="03_stan.html#installing-stan">Installing Stan</a></li>
<li><a href="03_stan.html#the-way-of-stanrstan">The Way of Stan/RStan</a></li>
<li class="has-sub"><a href="03_stan.html#elements-of-a-stan-program">Elements of a Stan Program</a><ul>
<li><a href="03_stan.html#data">Data</a></li>
<li><a href="03_stan.html#transformed-data">Transformed Data</a></li>
<li><a href="03_stan.html#parameters">Parameters</a></li>
<li><a href="03_stan.html#transformed-parameters">Transformed Parameters</a></li>
<li><a href="03_stan.html#model">Model</a></li>
<li><a href="03_stan.html#generated-quantities">Generated Quantities</a></li>
</ul></li>
<li><a href="03_stan.html#using-stan">Using Stan</a></li>
</ul></li>
<li class="has-sub"><a href="04_R.html#r">R</a><ul>
<li class="has-sub"><a href="04_R.html#rstan">rstan</a><ul>
<li><a href="04_R.html#data-list">Data list</a></li>
<li><a href="04_R.html#debug-model">Debug model</a></li>
<li><a href="04_R.html#full-model">Full model</a></li>
<li><a href="04_R.html#model-summary">Model summary</a></li>
<li><a href="04_R.html#diagnostics-and-beyond">Diagnostics and beyond</a></li>
</ul></li>
<li><a href="04_R.html#rstanarm">rstanarm</a></li>
<li><a href="04_R.html#brms">brms</a></li>
<li><a href="04_R.html#rethinking">rethinking</a></li>
<li><a href="04_R.html#summary">Summary</a></li>
</ul></li>
<li class="has-sub"><a href="05_extensions.html#extensions">Extensions</a><ul>
<li><a href="05_extensions.html#r-1">R</a></li>
<li><a href="05_extensions.html#stan-functions">Stan functions</a></li>
<li><a href="05_extensions.html#other-frameworks">Other frameworks</a></li>
</ul></li>
<li><a href="1000_Conclusion.html#conclusion">Conclusion</a></li>
<li><a href="1001_references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="key-concepts" class="section level1">
<h1>Key Concepts</h1>
<p>This section focuses on some key ideas to help you quickly get into the Bayesian mindset. Again, very little statistical knowledge is assumed. If you have prior experience with the Bayesian approach it may be skipped.</p>
<p><img src="img/priorLikePosterior_test1.png" style="display:block; margin: 0 auto;"></p>
<div id="distributions" class="section level2">
<h2>Distributions</h2>
<p>One of the first things to get used to coming from a traditional framework is that the focus is on distributions of parameters rather than single estimates of them. In the Bayesian context, parameters are <span class="emph"><em>random</em></span>, not fixed. Your analysis will start with one distribution, the <span class="emph">prior</span>, and end with another, the <span class="emph">posterior</span>. The summaries of that distribution, e.g. the mean, standard deviation, etc. will be available, and thus be used to understand the parameters in similar fashion as the traditional approach.</p>
<p>As an example, if you want to estimate a regression coefficient, the Bayesian analysis will result in hundreds to thousands of values from the distribution for that coefficient. You can then use those values to obtain their mean, or use the quantiles to provide an interval estimate, and thus end up with the same type of information.</p>
<p>Consider the following example. We obtain a 1000 draws from a normal distribution with mean 5 and standard deviation of 2. From those values we can get the mean or an interval estimate.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coef_result =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dv">5</span>, <span class="dv">2</span>)
<span class="kw">head</span>(coef_result)</code></pre></div>
<pre><code>[1] 5.412632 8.345243 5.046426 1.596307 6.881174 6.687836</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(coef_result)</code></pre></div>
<pre><code>[1] 5.081319</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(coef_result)</code></pre></div>
<pre><code>[1] 1.955895</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">quantile</span>(coef_result, <span class="kw">c</span>(.<span class="dv">025</span>,.<span class="dv">975</span>))</code></pre></div>
<pre><code>    2.5%    97.5% 
1.438507 8.916159 </code></pre>
<p>You will end up specifying the nature of that distribution depending on the model and goals of your situation, but the concept will be no different.</p>
</div>
<div id="prior" class="section level2">
<h2>Prior</h2>
<p>For the Bayesian approach we must choose a <span class="emph">prior distribution</span> representing our initial beliefs about the estimate. This is traditionally where some specifically have difficulty with Bayesian estimation, and newcomers are most wary. You will have to make choices here, but they are no different than the sort of choices you’ve always made in statistical modeling. Perhaps you let the program do it for you (generally a bad idea), or put little thought into it (also a bad idea), but such choices were always there. Examples include which variables go into the model, the nature of the likelihood (e.g. normal vs. Poisson), whether to include interactions, etc. You’re <em>always</em> making these choices in statistical modeling. <em>Always</em>. If it didn’t bother you before it needn’t now.</p>
<p>The prior’s settings are typically based on modeling concerns. As an example, the prior for regression coefficients could be set to uniform with some common sense boundaries. This would more or less defeat the purpose of the Bayesian approach, but it represents a situation in which we are completely ignorant of the situation. In fact, doing so would produce the results from standard likelihood regression approaches, and thus you can think of your familiar approach as a Bayesian one with uninformed priors. However, more common practice usually sets the prior for a regression coefficient to be normal, with mean at zero and with a relatively large variance. The large variance reflects our ignorance, but using the normal results in nicer estimation properties. The really nice thing is however, that we could have set the mean to that seen for the same or similar situations in prior research. In this case we can build upon the work that came before.</p>
<p>Setting aside the fact that such ‘subjectivity’ is an inherent part of the scientific process, and that ignoring prior information, if explicitly available from previous research, would be blatantly unscientific, the main point to make here is that this choice is not an <em>arbitrary</em> one. There are many distributions we might work with, but some will be better for us than others. And we can always test different priors to see how they might affect results (if at all)<label for="tufte-sn-2" class="margin-toggle sidenote-number">2</label><input type="checkbox" id="tufte-sn-2" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">2</span> Such testing is referred to as sensitivity analysis.</span>.</p>
</div>
<div id="likelihood" class="section level2">
<h2>Likelihood</h2>
<p>I won’t say too much about the <span class="emph">likelihood function</span> here. I have a refresher in the appendix of the <a href="http://m-clark.github.io/docs/IntroBayes.html">Bayesian Basics doc</a>. In any case here is a brief example. We’ll create a likelihood function for a standard regression setting, and compare results for two estimation situations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># likelihood function</span>
reg_ll =<span class="st"> </span>function(X, y, beta, sigma){
  <span class="kw">sum</span>(<span class="kw">dnorm</span>(y, <span class="dt">mean=</span>X%*%beta, <span class="dt">sd=</span>sigma, <span class="dt">log=</span>T))
}

<span class="co"># true values</span>
true_beta =<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">5</span>)
true_sigma =<span class="st"> </span><span class="dv">1</span>

<span class="co"># comparison values</span>
other_beta =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">3</span>)
other_sigma =<span class="st"> </span><span class="dv">2</span>

<span class="co"># sample size</span>
N =<span class="st"> </span><span class="dv">1000</span>

<span class="co"># data generation</span>
X =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, <span class="kw">runif</span>(N))
y =<span class="st"> </span>X %*%<span class="st"> </span>true_beta +<span class="st"> </span><span class="kw">rnorm</span>(N, <span class="dt">sd=</span>true_sigma)

<span class="co"># calculate likelihooods</span>
<span class="kw">reg_ll</span>(X, y, <span class="dt">beta=</span>true_beta, <span class="dt">sigma=</span>true_sigma)    <span class="co"># more likely</span></code></pre></div>
<pre><code>[1] -1389.567</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">reg_ll</span>(X, y, <span class="dt">beta=</span>other_beta, <span class="dt">sigma=</span>other_sigma)  <span class="co"># less likely</span></code></pre></div>
<pre><code>[1] -2888.643</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">logLik</span>(<span class="kw">lm</span>(y~., <span class="dt">data=</span><span class="kw">data.frame</span>(X[,-<span class="dv">1</span>])))          <span class="co"># actual log likelihood</span></code></pre></div>
<pre><code>&#39;log Lik.&#39; -1388.289 (df=3)</code></pre>
<p>The above demonstrates a couple things. The likelihood tells us the relative number of ways the data could occur given the parameter estimates. For a standard linear model where the likelihood is based on a normal distribution, we require estimates for the coefficients and the variance/standard deviation. In the standard maximum likelihood (ML) approach commonly used in statistical analysis, we use an iterative process to end up with estimates of the parameters that maximize the data likelihood. With more data, and a lot of other considerations going in our favor, we end up closer to the true values<label for="tufte-sn-3" class="margin-toggle sidenote-number">3</label><input type="checkbox" id="tufte-sn-3" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">3</span> Assuming such a thing exists.</span>. A key difference from standard ML methods and the Bayesian approach is that the former assumes a fixed parameter, while the Bayesian approach assumes the parameter is <em>random</em>.</p>
</div>
<div id="posterior" class="section level2">
<h2>Posterior</h2>
<p>The <span class="emph">posterior</span> distribution is a weighted combination of the prior and the likelihood, and is proportional to their product. Assuming some <span class="math inline">\(\theta\)</span> is the parameter of interest:</p>
<p><span class="math display">\[ p(\theta|Data) \propto p(Data|\theta) \cdot p(\theta) \]</span> <span class="math display">\[ \;\;\mathrm{posterior} \;\propto \mathrm{likelihood} \;\!\cdot \mathrm{prior} \]</span> With more data, i.e. evidence, the weight shifts ever more to the likelihood, ultimately rendering the prior inconsequential. Let’s now <a href="http://micl.shinyapps.io/prior2post/">see this in action</a>.</p>
</div>
<div id="p-values" class="section level2">
<h2>P-values</h2>
<p>One of the many nice things about the Bayesian approach regards the probabilities and intervals we obtain, specifically their interpretation. For some of you still new to statistical analysis in general, this may be the interpretation you were already using, though incorrectly.</p>
<p>As an example, the p-value from standard <span class="emph">null hypothesis testing</span> goes something like the following:</p>
<blockquote>
<p><em>If the null hypothesis is true</em>, the probability of seeing a result like this or more extreme is P.</p>
</blockquote>
<p>Contrast this with the following:</p>
<blockquote>
<p>The probability of this result being different from zero is P.</p>
</blockquote>
<p>Which is more straightforward? Now consider the interval estimates:</p>
<blockquote>
<p class="fragment">
If I repeat this study precisely an infinite number of times, and I calculate a P% interval each time, then P% of those intervals will contain the true parameter.
</p>
</blockquote>
<p>Or:</p>
<blockquote>
<p class="fragment">
P is the probability the parameter falls in <em>this</em> interval.
</p>
</blockquote>
<p>One of these reflects the notions and language we use in everyday speech, the other hasn’t been understood very well by most people practicing science. While oftentimes, and especially for simpler models, a Bayesian interval will not be much different than the traditional one, at least it will be something you could describe to the average Joe on the street. In addition, you can generate distributions, and thus estimates with corresponding intervals, for anything you can calculate based on the model. Such statistics are a natural by-product of the approach, and make it easier to explore your data further.</p>

</div>
</div>
<p style="text-align: center;">
<a href="01_intro.html"><button class="btn btn-default">Previous</button></a>
<a href="03_stan.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
