---
title: "Notes to self"
description: |
  So I don't forget
author:
  - name: Michael Clark
    url: https://m-clark.github.io
date:  '`r format(Sys.Date(), "%B %d, %Y")`'
preview: ../../img/priorLikePosterior.png   # apparently no way to change the size displayed via css (ignored) or file (stretched)
output:
  distill::distill_article:
    self_contained: false
    toc: true
    css: ../../styles.css
draft: true
tags: [gausian process, GAM]
categories:
  - regression
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  message = FALSE, 
  warning = FALSE, 
  comment = NA,
  R.options = list(width = 120),
  cache.rebuild = FALSE,
  cache = TRUE, 
  fig.align='center', 
  dev = 'svg', 
  dev.args=list(bg = 'transparent')
)

library(tidyverse)
library(tidyext)
library(broom)
library(kableExtra)
library(visibly)

kable_df <- function(..., digits=3) {
  kable(..., digits=digits) %>% 
    kable_styling(full_width = F)
}
```

## Intro

This is where I will continuously add notes/blurbs that may eventually find more detailed exposition, but more so are just things for me to remember or think about.


## Gaussian Processes, relationships

I had this info on my whiteboard for a long time thinking I'd get back to it at some point, but never really did.  A while back, I had started a [project](https://github.com/m-clark/connections) regarding the various connections among common modeling scenarios, but abandoned it when the package it relied on made fundamental, breaking changes without documentation or deprecation. There I had a [section on gaussian process](https://github.com/m-clark/connections/blob/master/gp.Rmd) and relations to typical time-series and GAM models.

```{r gp-diagrammer, echo=FALSE, eval=FALSE}
library(DiagrammeR)
gp_nodes = #create_node_df(
  tibble(
  n = 15,
  id = c('gpm','gpsq', 'gpexp', 'gprq', 'gplin', 'gpother', 'GP', 
            'rkhs','ar1','ou', 'splines', 'GAM', 'SVM', 'NN', 'bayesreg'),
  label = c('GP w/ Matern', 'GP w/ SqExp', 'GP w/ Exp', 'GP w/ RQ', 'GP w/ linear', 'GP w/ Other', 'GP', 
            'RKHS', 'AR(p)', 'OU', 'Splines', 'GAM', 'SVM', 'NN', 'Bayesian\nRegression'),
  type = 'a',
  # value = 1,
  
  # style = 'filled',
  # shape = 'circle',
  # # fixedsize=T,
  # distortion='',
  # color=c(rep('#8B000040', 6), 'lightblue', rep('#8B000040', 20)),
  # fillcolor='papayawhip',
  # fixedsize='',
  # fontcolor='darkred',
  # fontname='Roboto',
  # fontsize=6,
  # height='',
  # penwidth='',
  # peripheries=c(rep('1', 6), '4'),
  # sides='',
  # tooltip='',
  # width='',
  x='',
  y='')

gp_edges = # create_edge_df(
  tibble(
  from = c('gpm','ou', 'gpm','gpexp','gpm','rkhs','GP', 'gprq','GP', 'gpother', 
           'SVM', 'NN', 'gpother', 'GAM', 'gpsq', 'GP', 'gplin', 'GP'),
  to =   c('ar1','ar1', 'gpsq', 'ou','gpexp', 'GP', 'gpm','gpsq', 'gpother', 'splines', 
           'gpother', 'gpother', 'GAM', 'splines', 'gpexp', 'gplin', 'bayesreg', 'gprq'),
  rel = c('a'),
  arrowhead='dot',
  arrowsize='.5',
  arrowtail='',
  color= '#FA8072BF', #'#8B000040',
  dir=c('', '','', 'none', '', 'none', '', '', '', '', 
        'none', 'none', '', '', '', '', 'none', ''),
  fontcolor='',
  fontname='',
  fontsize='',
  headport='',
  label=rep('&nbsp;', length=18),
  minlen='',
  penwidth='2',
  tailport='',
  tooltip=c('Matern with &nu; fixed to p-1/2', 
            'AR(p) is discretized version of OU', 
            'Matern with &nu; = &infin;',
            '&equiv;',
            'Matern with &nu; = 1/2',
            '',
            '',
            'RQ is a mixture of SQExp; &alpha; = &infin;',
            '',
            '',
            '',
            '',
            'thin plate, duchon spline',
            '',
            '',
            '',
            '&equiv;',
            '')
)

gp_nodes
gp_edges %>% select(from, to, tooltip)

# create_graph(gp_nodes, gp_edges, graph_name='Gaussian Processes')
```



### Covariance Functions

Note that covariance functions are written identically in slightly different ways in different sources, and this is just one representation.  See the Rasmussen and Williams link for details, which the following adheres to.

#### Exponential

Or $\gamma$-exponential:

$$k_{\gamma} = exp(-(\frac{r}{l})^\gamma$$


#### Squared Exponential

$$k_{se} = exp(-\frac{r^2}{2l^2})$$


#### Rational Quadratic

A mixture of squared exponentials.

$$k_{rq} = (1 + \frac{r^2}{2\alpha l^2})^{-\alpha}$$


#### Matern class



$$h^2\frac{2^{1-\nu}}{\Gamma(\nu)}(2\sqrt{\nu}\frac{r}{l})\mathcal{B}_\nu(2\sqrt{\nu}\frac{r}{l})$$


$r$ = $|x - x'|$

$l$ = horizontal/input length-scale 'length of wiggles'

$h$ = vertical/output length-scale  (variance, i.e. distance from mean)

$\nu$ = controls differentiability

$\Gamma$ = Gamma function

$\mathcal{B}$ = modified Bessel function of the second kind



### Connections


- AR(1) = GP with matern covariance and $\nu$ fixed to 1/2 (i.e. $\nu+1/2 = p$)
- AR(p) = is a discretized Ornsteinâ€“Uhlenbeck,  GP with matern covariance and nu fixed to p - 1/2
- GP with SE = matern with $\nu = \infty$
- GP with RQ = a mixture of GP with SE, and if $\alpha = \infty$ = GP with SE
- GP with Exponential = matern with $\nu = 1/2$
- GP $\leadsto$ Thinplate, Duchon spline for a GAM, other splines
- GP with linear kernel $\sigma_f + (x-c)(x'-c)$ = Bayesian linear regression
- GP $\equiv$ posterior mean for RKHS
- GP = Fully connected neural net with infinitely wide hidden layer
- Other: SVM

### References

- Rasmussen & Williams (2006). [Gaussian Processes for Machine Learning](http://www.gaussianprocess.org/gpml).
- Murphy (2012). Machine Learning: A probabilistic perspective.
- [The kernel cookbook](http://www.cs.toronto.edu/~duvenaud/cookbook/); [pdf chapter](https://raw.githubusercontent.com/duvenaud/phd-thesis/master/kernels.pdf)

#### Stack 

- [AR-OU](http://math.stackexchange.com/questions/345773/how-the-ornstein-uhlenbeck-process-can-be-considered-as-the-continuous-time-anal)
- [Covariance Functions](https://en.wikipedia.org/wiki/Gaussian_process#Usual_covariance_functions)


## Marginal effects in brms